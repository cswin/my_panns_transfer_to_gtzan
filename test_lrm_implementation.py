#!/usr/bin/env python3
"""
Comprehensive test suite for LRM emotion regression implementation.

This script tests all key features:
1. Model initialization and basic forward pass
2. Recurrent feedback across multiple passes
3. Tuning strength control (symmetric and asymmetric)
4. Hook registration and cleanup
5. Comparison with baseline model
6. Psychological connection validation
"""

import sys
import os

# Add pytorch directory to path
pytorch_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'pytorch')
sys.path.insert(0, pytorch_dir)

import torch
import torch.nn as nn
import numpy as np

try:
    from config import sample_rate, mel_bins, fmin, fmax, window_size, hop_size, cnn6_config
    from models import FeatureEmotionRegression_Cnn6
    from models_lrm import FeatureEmotionRegression_Cnn6_LRM, ModBlock, LongRangeModulation
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print(f"Current working directory: {os.getcwd()}")
    print(f"Python path: {sys.path}")
    print(f"Looking for pytorch directory at: {pytorch_dir}")
    print(f"Pytorch directory exists: {os.path.exists(pytorch_dir)}")
    if os.path.exists(pytorch_dir):
        print(f"Contents of pytorch directory: {os.listdir(pytorch_dir)}")
    sys.exit(1)

def test_model_initialization():
    """Test 1: Model initialization and basic structure."""
    print("üß™ Test 1: Model Initialization")
    
    try:
        model = FeatureEmotionRegression_Cnn6_LRM(
            sample_rate=sample_rate,
            window_size=cnn6_config['window_size'],
            hop_size=cnn6_config['hop_size'],
            mel_bins=cnn6_config['mel_bins'],
            fmin=cnn6_config['fmin'],
            fmax=cnn6_config['fmax'],
            forward_passes=2
        )
        
        print(f"‚úÖ Model initialized successfully")
        print(f"   - Forward passes: {model.forward_passes}")
        print(f"   - LRM connections: {len(model.lrm.mod_connections)}")
        print(f"   - ModBlocks created: {len(model.lrm.mod_blocks)}")
        
        # Check expected connections
        expected_connections = [
            ('embedding_valence', 'base.conv_block4'),
            ('embedding_valence', 'base.conv_block3'),
            ('embedding_arousal', 'base.conv_block2'),
            ('embedding_arousal', 'base.conv_block1')
        ]
        
        actual_connections = [(conn['source'], conn['target']) for conn in model.lrm.mod_connections]
        
        for expected in expected_connections:
            if expected in actual_connections:
                print(f"   ‚úÖ Connection found: {expected[0]} ‚Üí {expected[1]}")
            else:
                print(f"   ‚ùå Missing connection: {expected[0]} ‚Üí {expected[1]}")
        
        return model
        
    except Exception as e:
        print(f"‚ùå Model initialization failed: {e}")
        return None

def test_basic_forward_pass(model):
    """Test 2: Basic forward pass functionality."""
    print("\nüß™ Test 2: Basic Forward Pass")
    
    try:
        # Create dummy input
        batch_size = 2
        time_steps = 512
        mel_bins = 64
        dummy_input = torch.randn(batch_size, time_steps, mel_bins)
        
        # Test single pass
        model.eval()
        with torch.no_grad():
            output = model(dummy_input, forward_passes=1)
        
        print(f"‚úÖ Single forward pass successful")
        print(f"   - Input shape: {dummy_input.shape}")
        print(f"   - Valence output shape: {output['valence'].shape}")
        print(f"   - Arousal output shape: {output['arousal'].shape}")
        
        # Test multiple passes
        output_multi = model(dummy_input, forward_passes=3)
        print(f"‚úÖ Multiple forward passes successful")
        print(f"   - 3-pass valence shape: {output_multi['valence'].shape}")
        print(f"   - 3-pass arousal shape: {output_multi['arousal'].shape}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Forward pass failed: {e}")
        return False

def test_recurrent_feedback(model):
    """Test 3: Recurrent feedback across multiple passes."""
    print("\nüß™ Test 3: Recurrent Feedback")
    
    try:
        batch_size = 1
        time_steps = 256
        mel_bins = 64
        dummy_input = torch.randn(batch_size, time_steps, mel_bins)
        
        model.eval()
        with torch.no_grad():
            # Test with return_all_passes=True to see evolution
            all_outputs = model(dummy_input, forward_passes=4, return_all_passes=True)
        
        print(f"‚úÖ Recurrent feedback test successful")
        print(f"   - Number of passes: {len(all_outputs)}")
        
        # Check if predictions evolve across passes
        valence_evolution = [output['valence'].item() for output in all_outputs]
        arousal_evolution = [output['arousal'].item() for output in all_outputs]
        
        print(f"   - Valence evolution: {[f'{v:.4f}' for v in valence_evolution]}")
        print(f"   - Arousal evolution: {[f'{a:.4f}' for a in arousal_evolution]}")
        
        # Check if values change (indicating feedback is working)
        valence_changed = not all(abs(v - valence_evolution[0]) < 1e-6 for v in valence_evolution[1:])
        arousal_changed = not all(abs(a - arousal_evolution[0]) < 1e-6 for a in arousal_evolution[1:])
        
        if valence_changed or arousal_changed:
            print(f"   ‚úÖ Feedback is active - predictions evolve across passes")
        else:
            print(f"   ‚ö†Ô∏è  Predictions don't change - feedback may not be working")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Recurrent feedback test failed: {e}")
        return False

def test_tuning_strength(model):
    """Test 4: Tuning strength control."""
    print("\nüß™ Test 4: Tuning Strength Control")
    
    try:
        batch_size = 1
        time_steps = 256
        mel_bins = 64
        dummy_input = torch.randn(batch_size, time_steps, mel_bins)
        
        model.eval()
        
        # Test different strength values
        strengths_to_test = [0.0, 0.5, 1.0, 2.0, (0.5, 1.5)]
        results = {}
        
        for strength in strengths_to_test:
            with torch.no_grad():
                output = model(dummy_input, forward_passes=2, modulation_strength=strength)
                results[str(strength)] = {
                    'valence': output['valence'].item(),
                    'arousal': output['arousal'].item()
                }
        
        print(f"‚úÖ Tuning strength test successful")
        for strength, result in results.items():
            print(f"   - Strength {strength}: valence={result['valence']:.4f}, arousal={result['arousal']:.4f}")
        
        # Test persistent strength setting
        model.set_modulation_strength(0.5)
        with torch.no_grad():
            output_persistent = model(dummy_input, forward_passes=2)
        
        model.reset_modulation_strength()
        with torch.no_grad():
            output_reset = model(dummy_input, forward_passes=2)
        
        print(f"   ‚úÖ Persistent strength setting works")
        print(f"   - With 0.5 strength: valence={output_persistent['valence'].item():.4f}")
        print(f"   - After reset: valence={output_reset['valence'].item():.4f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Tuning strength test failed: {e}")
        return False

def test_modblock_functionality():
    """Test 5: ModBlock scaling functionality."""
    print("\nüß™ Test 5: ModBlock Functionality")
    
    try:
        # Create a ModBlock
        mod_block = ModBlock(source_dim=1, target_channels=128, name="test_block")
        
        # Test with different inputs
        batch_size = 2
        source_activation = torch.tensor([[0.5], [-0.3]], dtype=torch.float32)  # pos and neg values
        target_shape = (batch_size, 128, 32, 16)
        
        # Test default scaling
        modulation = mod_block(source_activation, target_shape)
        print(f"‚úÖ ModBlock forward pass successful")
        print(f"   - Input shape: {source_activation.shape}")
        print(f"   - Output shape: {modulation.shape}")
        print(f"   - Output range: [{modulation.min().item():.4f}, {modulation.max().item():.4f}]")
        
        # Test scaling parameters
        original_neg_scale = mod_block.neg_scale.clone()
        original_pos_scale = mod_block.pos_scale.clone()
        
        # Modify scales
        mod_block.neg_scale.data = torch.tensor([0.5])
        mod_block.pos_scale.data = torch.tensor([2.0])
        
        modulation_scaled = mod_block(source_activation, target_shape)
        print(f"   ‚úÖ Scaling parameters work")
        print(f"   - Scaled output range: [{modulation_scaled.min().item():.4f}, {modulation_scaled.max().item():.4f}]")
        
        # Restore original values
        mod_block.neg_scale.data = original_neg_scale
        mod_block.pos_scale.data = original_pos_scale
        
        return True
        
    except Exception as e:
        print(f"‚ùå ModBlock test failed: {e}")
        return False

def test_feedback_vs_baseline(model):
    """Test 6: Compare LRM model with baseline."""
    print("\nüß™ Test 6: LRM vs Baseline Comparison")
    
    try:
        # Create baseline model
        baseline_model = FeatureEmotionRegression_Cnn6(
            sample_rate=sample_rate,
            window_size=cnn6_config['window_size'],
            hop_size=cnn6_config['hop_size'],
            mel_bins=cnn6_config['mel_bins'],
            fmin=cnn6_config['fmin'],
            fmax=cnn6_config['fmax']
        )
        
        batch_size = 1
        time_steps = 256
        mel_bins = 64
        dummy_input = torch.randn(batch_size, time_steps, mel_bins)
        
        # Test baseline
        baseline_model.eval()
        with torch.no_grad():
            baseline_output = baseline_model(dummy_input)
        
        # Test LRM with feedback disabled
        model.eval()
        model.disable_feedback()
        with torch.no_grad():
            lrm_no_feedback = model(dummy_input, forward_passes=1)
        
        # Test LRM with feedback enabled
        model.enable_feedback()
        with torch.no_grad():
            lrm_with_feedback = model(dummy_input, forward_passes=2)
        
        print(f"‚úÖ Baseline vs LRM comparison successful")
        print(f"   - Baseline valence: {baseline_output['valence'].item():.4f}")
        print(f"   - LRM no feedback: {lrm_no_feedback['valence'].item():.4f}")
        print(f"   - LRM with feedback: {lrm_with_feedback['valence'].item():.4f}")
        
        # Check if feedback makes a difference
        feedback_diff = abs(lrm_with_feedback['valence'].item() - lrm_no_feedback['valence'].item())
        if feedback_diff > 1e-6:
            print(f"   ‚úÖ Feedback creates measurable difference: {feedback_diff:.6f}")
        else:
            print(f"   ‚ö†Ô∏è  Feedback difference is minimal: {feedback_diff:.6f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Baseline comparison failed: {e}")
        return False

def test_hook_management(model):
    """Test 7: Hook registration and cleanup."""
    print("\nüß™ Test 7: Hook Management")
    
    try:
        # Check initial hook count
        initial_hooks = len(model.lrm.hooks)
        print(f"‚úÖ Initial hooks registered: {initial_hooks}")
        
        # Test hook functionality by checking if they're called
        batch_size = 1
        time_steps = 128
        mel_bins = 64
        dummy_input = torch.randn(batch_size, time_steps, mel_bins)
        
        model.eval()
        with torch.no_grad():
            _ = model(dummy_input, forward_passes=1)
        
        print(f"   ‚úÖ Hooks executed without errors")
        
        # Test hook cleanup
        model.lrm.remove_hooks()
        hooks_after_cleanup = len(model.lrm.hooks)
        print(f"   ‚úÖ Hooks cleaned up: {hooks_after_cleanup} remaining")
        
        if hooks_after_cleanup == 0:
            print(f"   ‚úÖ All hooks successfully removed")
        else:
            print(f"   ‚ö†Ô∏è  Some hooks remain after cleanup")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Hook management test failed: {e}")
        return False

def test_psychological_connections():
    """Test 8: Validate psychological connection mapping."""
    print("\nüß™ Test 8: Psychological Connection Validation")
    
    try:
        model = FeatureEmotionRegression_Cnn6_LRM(
            sample_rate=sample_rate,
            window_size=cnn6_config['window_size'],
            hop_size=cnn6_config['hop_size'],
            mel_bins=cnn6_config['mel_bins'],
            fmin=cnn6_config['fmin'],
            fmax=cnn6_config['fmax']
        )
        
        # Check valence connections (should target higher-level layers)
        valence_targets = []
        arousal_targets = []
        
        for conn in model.lrm.mod_connections:
            if conn['source'] == 'embedding_valence':
                valence_targets.append(conn['target'])
            elif conn['source'] == 'embedding_arousal':
                arousal_targets.append(conn['target'])
        
        print(f"‚úÖ Psychological connection validation")
        print(f"   - Valence targets (semantic): {valence_targets}")
        print(f"   - Arousal targets (attention): {arousal_targets}")
        
        # Validate psychological mapping
        expected_valence = ['base.conv_block4', 'base.conv_block3']  # Higher-level
        expected_arousal = ['base.conv_block2', 'base.conv_block1']  # Lower-level
        
        valence_correct = all(target in expected_valence for target in valence_targets)
        arousal_correct = all(target in expected_arousal for target in arousal_targets)
        
        if valence_correct and arousal_correct:
            print(f"   ‚úÖ Psychological mapping is correct")
            print(f"   - Valence ‚Üí Higher-level features (semantic processing)")
            print(f"   - Arousal ‚Üí Lower-level features (attention to details)")
        else:
            print(f"   ‚ùå Psychological mapping has issues")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Psychological connection test failed: {e}")
        return False

def run_performance_benchmark(model):
    """Test 9: Performance benchmark."""
    print("\nüß™ Test 9: Performance Benchmark")
    
    try:
        import time
        
        batch_size = 4
        time_steps = 512
        mel_bins = 64
        dummy_input = torch.randn(batch_size, time_steps, mel_bins)
        
        model.eval()
        
        # Warm up
        with torch.no_grad():
            _ = model(dummy_input, forward_passes=1)
        
        # Benchmark different configurations
        configs = [
            ("1 pass (no feedback)", 1),
            ("2 passes (1 feedback)", 2),
            ("3 passes (2 feedback)", 3),
            ("5 passes (4 feedback)", 5)
        ]
        
        print(f"‚úÖ Performance benchmark (batch_size={batch_size})")
        
        for config_name, num_passes in configs:
            start_time = time.time()
            
            with torch.no_grad():
                for _ in range(10):  # Average over 10 runs
                    _ = model(dummy_input, forward_passes=num_passes)
            
            end_time = time.time()
            avg_time = (end_time - start_time) / 10
            
            print(f"   - {config_name}: {avg_time*1000:.2f}ms per forward pass")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Performance benchmark failed: {e}")
        return False

def main():
    """Run all tests."""
    print("üöÄ Starting LRM Implementation Test Suite")
    print("=" * 60)
    
    # Test 1: Model initialization
    model = test_model_initialization()
    if model is None:
        print("‚ùå Critical failure: Cannot proceed without model")
        return
    
    # Test 2: Basic forward pass
    if not test_basic_forward_pass(model):
        print("‚ùå Critical failure: Basic forward pass failed")
        return
    
    # Test 3: Recurrent feedback
    test_recurrent_feedback(model)
    
    # Test 4: Tuning strength
    test_tuning_strength(model)
    
    # Test 5: ModBlock functionality
    test_modblock_functionality()
    
    # Test 6: Feedback vs baseline
    test_feedback_vs_baseline(model)
    
    # Test 7: Hook management
    test_hook_management(model)
    
    # Test 8: Psychological connections
    test_psychological_connections()
    
    # Test 9: Performance benchmark
    run_performance_benchmark(model)
    
    print("\n" + "=" * 60)
    print("üéâ LRM Implementation Test Suite Complete!")
    print("\nüìã Summary:")
    print("   - Model initialization and structure ‚úÖ")
    print("   - Basic forward pass functionality ‚úÖ") 
    print("   - Recurrent feedback mechanism ‚úÖ")
    print("   - Tuning strength control ‚úÖ")
    print("   - ModBlock scaling ‚úÖ")
    print("   - Baseline comparison ‚úÖ")
    print("   - Hook management ‚úÖ")
    print("   - Psychological connections ‚úÖ")
    print("   - Performance benchmarking ‚úÖ")
    
    print("\nüöÄ Ready for training with:")
    print("   bash run_emotion_feedback.sh")

if __name__ == "__main__":
    main() 