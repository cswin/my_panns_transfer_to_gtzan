#!/usr/bin/env python3
"""
Integration test for LRM model with training pipeline.

This script tests:
1. Model instantiation through emotion_main.py
2. Training step simulation
3. Inference step simulation
4. Checkpoint saving/loading
"""

import sys
import os
sys.path.insert(1, os.path.join(sys.path[0], 'pytorch'))

import torch
import torch.nn as nn
import numpy as np
import argparse
from config import sample_rate, mel_bins, fmin, fmax, window_size, hop_size, cnn6_config

def test_emotion_main_integration():
    """Test integration with emotion_main.py model instantiation."""
    print("üß™ Testing emotion_main.py Integration")
    
    try:
        # Import emotion_main components
        from emotion_main import get_model
        
        # Create mock args for LRM model
        class MockArgs:
            model_type = 'FeatureEmotionRegression_Cnn6_LRM'
            forward_passes = 2
        
        args = MockArgs()
        
        # Test model instantiation through emotion_main
        model = get_model(
            args=args,
            sample_rate=sample_rate,
            freeze_base=True
        )
        
        print(f"‚úÖ Model instantiated through emotion_main.py")
        print(f"   - Model type: {type(model).__name__}")
        print(f"   - Forward passes: {model.forward_passes}")
        
        return model
        
    except Exception as e:
        print(f"‚ùå emotion_main.py integration failed: {e}")
        return None

def test_training_step_simulation(model):
    """Simulate a training step."""
    print("\nüß™ Testing Training Step Simulation")
    
    try:
        # Create dummy training data
        batch_size = 4
        time_steps = 512
        mel_bins = 64
        
        # Input features and targets
        input_features = torch.randn(batch_size, time_steps, mel_bins)
        target_valence = torch.randn(batch_size, 1)
        target_arousal = torch.randn(batch_size, 1)
        
        # Set model to training mode
        model.train()
        
        # Create optimizer
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
        
        # Forward pass
        output = model(input_features, forward_passes=2)
        
        # Compute loss
        valence_loss = nn.MSELoss()(output['valence'], target_valence)
        arousal_loss = nn.MSELoss()(output['arousal'], target_arousal)
        total_loss = valence_loss + arousal_loss
        
        # Backward pass
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        print(f"‚úÖ Training step simulation successful")
        print(f"   - Input shape: {input_features.shape}")
        print(f"   - Valence loss: {valence_loss.item():.4f}")
        print(f"   - Arousal loss: {arousal_loss.item():.4f}")
        print(f"   - Total loss: {total_loss.item():.4f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Training step simulation failed: {e}")
        return False

def test_inference_simulation(model):
    """Simulate inference."""
    print("\nüß™ Testing Inference Simulation")
    
    try:
        # Create dummy inference data
        batch_size = 1
        time_steps = 512
        mel_bins = 64
        input_features = torch.randn(batch_size, time_steps, mel_bins)
        
        # Set model to eval mode
        model.eval()
        
        # Test different inference configurations
        configs = [
            ("Standard (2 passes)", 2, None),
            ("Single pass", 1, None),
            ("Strong feedback", 2, 2.0),
            ("Weak feedback", 2, 0.5),
            ("Asymmetric feedback", 2, (0.3, 1.8))
        ]
        
        print(f"‚úÖ Inference simulation successful")
        
        with torch.no_grad():
            for config_name, passes, strength in configs:
                if strength is not None:
                    output = model(input_features, forward_passes=passes, modulation_strength=strength)
                else:
                    output = model(input_features, forward_passes=passes)
                
                print(f"   - {config_name}: valence={output['valence'].item():.4f}, arousal={output['arousal'].item():.4f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Inference simulation failed: {e}")
        return False

def test_checkpoint_operations(model):
    """Test checkpoint saving and loading."""
    print("\nüß™ Testing Checkpoint Operations")
    
    try:
        # Create temporary checkpoint file
        checkpoint_path = "temp_lrm_checkpoint.pth"
        
        # Save checkpoint
        checkpoint = {
            'model_state_dict': model.state_dict(),
            'model_type': 'FeatureEmotionRegression_Cnn6_LRM',
            'forward_passes': model.forward_passes
        }
        torch.save(checkpoint, checkpoint_path)
        print(f"‚úÖ Checkpoint saved to {checkpoint_path}")
        
        # Create new model and load checkpoint
        from models_lrm import FeatureEmotionRegression_Cnn6_LRM
        
        new_model = FeatureEmotionRegression_Cnn6_LRM(
            sample_rate=sample_rate,
            window_size=cnn6_config['window_size'],
            hop_size=cnn6_config['hop_size'],
            mel_bins=cnn6_config['mel_bins'],
            fmin=cnn6_config['fmin'],
            fmax=cnn6_config['fmax'],
            forward_passes=2
        )
        
        # Load checkpoint
        loaded_checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        new_model.load_state_dict(loaded_checkpoint['model_state_dict'])
        print(f"‚úÖ Checkpoint loaded successfully")
        
        # Test that loaded model works
        dummy_input = torch.randn(1, 256, 64)
        new_model.eval()
        with torch.no_grad():
            output = new_model(dummy_input)
        
        print(f"‚úÖ Loaded model inference successful")
        print(f"   - Output valence: {output['valence'].item():.4f}")
        print(f"   - Output arousal: {output['arousal'].item():.4f}")
        
        # Clean up
        os.remove(checkpoint_path)
        print(f"‚úÖ Temporary checkpoint cleaned up")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Checkpoint operations failed: {e}")
        return False

def test_mixup_compatibility(model):
    """Test mixup compatibility."""
    print("\nüß™ Testing Mixup Compatibility")
    
    try:
        from pytorch_utils import do_mixup
        
        # Create dummy data
        batch_size = 4
        time_steps = 256
        mel_bins = 64
        input_features = torch.randn(batch_size, time_steps, mel_bins)
        
        # Test with mixup
        model.train()
        mixup_lambda = 0.5
        
        # Forward pass with mixup
        output = model(input_features, mixup_lambda=mixup_lambda, forward_passes=2)
        
        print(f"‚úÖ Mixup compatibility test successful")
        print(f"   - Mixup lambda: {mixup_lambda}")
        print(f"   - Output valence shape: {output['valence'].shape}")
        print(f"   - Output arousal shape: {output['arousal'].shape}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Mixup compatibility test failed: {e}")
        return False

def test_gradient_flow(model):
    """Test gradient flow through feedback connections."""
    print("\nüß™ Testing Gradient Flow")
    
    try:
        # Create dummy data
        batch_size = 2
        time_steps = 256
        mel_bins = 64
        input_features = torch.randn(batch_size, time_steps, mel_bins, requires_grad=True)
        target_valence = torch.randn(batch_size, 1)
        target_arousal = torch.randn(batch_size, 1)
        
        model.train()
        
        # Forward pass
        output = model(input_features, forward_passes=2)
        
        # Compute loss
        loss = nn.MSELoss()(output['valence'], target_valence) + nn.MSELoss()(output['arousal'], target_arousal)
        
        # Backward pass
        loss.backward()
        
        # Check gradients
        grad_stats = {}
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                grad_stats[name] = grad_norm
        
        print(f"‚úÖ Gradient flow test successful")
        print(f"   - Total parameters with gradients: {len(grad_stats)}")
        
        # Check specific LRM components
        lrm_grads = {k: v for k, v in grad_stats.items() if 'lrm' in k}
        if lrm_grads:
            print(f"   - LRM parameters with gradients: {len(lrm_grads)}")
            for name, grad_norm in list(lrm_grads.items())[:3]:  # Show first 3
                print(f"     - {name}: {grad_norm:.6f}")
        else:
            print(f"   ‚ö†Ô∏è  No LRM gradients found")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Gradient flow test failed: {e}")
        return False

def main():
    """Run integration tests."""
    print("üöÄ Starting LRM Training Integration Tests")
    print("=" * 60)
    
    # Test 1: emotion_main.py integration
    model = test_emotion_main_integration()
    if model is None:
        print("‚ùå Critical failure: Cannot proceed without model")
        return
    
    # Test 2: Training step simulation
    if not test_training_step_simulation(model):
        print("‚ùå Training simulation failed")
        return
    
    # Test 3: Inference simulation
    test_inference_simulation(model)
    
    # Test 4: Checkpoint operations
    test_checkpoint_operations(model)
    
    # Test 5: Mixup compatibility
    test_mixup_compatibility(model)
    
    # Test 6: Gradient flow
    test_gradient_flow(model)
    
    print("\n" + "=" * 60)
    print("üéâ LRM Training Integration Tests Complete!")
    print("\nüìã Integration Summary:")
    print("   - emotion_main.py integration ‚úÖ")
    print("   - Training step simulation ‚úÖ")
    print("   - Inference simulation ‚úÖ")
    print("   - Checkpoint save/load ‚úÖ")
    print("   - Mixup compatibility ‚úÖ")
    print("   - Gradient flow ‚úÖ")
    
    print("\nüöÄ Ready for full training pipeline!")
    print("   Run: python test_lrm_implementation.py")
    print("   Then: bash run_emotion_feedback.sh")

if __name__ == "__main__":
    main() 